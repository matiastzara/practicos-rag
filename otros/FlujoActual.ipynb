{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flujo completo RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from dotenv import load_dotenv\n",
    "from datasets import Dataset\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_qdrant import FastEmbedSparse, RetrievalMode\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "#from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from qdrant_client import QdrantClient\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# --- PDF Processing ---\n",
    "def load_pdf_all_documents(directory_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Carga documentos PDF desde una carpeta y devuelve una lista de páginas como texto.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Ruta de la carpeta que contiene los archivos PDF.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Lista de cadenas de texto, donde cada cadena corresponde al texto extraído de una página PDF.\n",
    "    \"\"\"\n",
    "    from PyPDF2 import PdfReader\n",
    "\n",
    "    all_texts = list()\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            try:\n",
    "                logging.info(f\"Cargando archivo: {file_path}\")\n",
    "                reader = PdfReader(file_path)\n",
    "                for page in reader.pages:\n",
    "                    all_texts.append(page.extract_text())\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error leyendo el archivo PDF: {file_path}. Detalle: {e}\")\n",
    "    return all_texts\n",
    "\n",
    "# --- Text Processing ---\n",
    "def clean_text_and_exclude_sections(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpia el texto eliminando espacios redundantes y caracteres especiales.\n",
    "\n",
    "    Args:\n",
    "        text (str): Texto a limpiar.\n",
    "\n",
    "    Returns:\n",
    "        str: Texto limpio con espacios redundantes eliminados.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Reemplazar múltiples espacios\n",
    "    return text.strip()\n",
    "\n",
    "# Función para dividir texto en oraciones\n",
    "def split_text_into_sentences(text: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Divide un texto en oraciones basado en '.', '?', y '!' y devuelve una lista de diccionarios.\n",
    "    Args:\n",
    "        text (str): El texto a dividir.\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: Lista de diccionarios con 'sentence' y 'index'.\n",
    "    \"\"\"\n",
    "    single_sentences_list = re.split(r'(?<=[.?!])\\s+', text.strip())\n",
    "    sentences = [{'sentence': sentence, 'index': i} for i, sentence in enumerate(single_sentences_list)]\n",
    "    return sentences\n",
    "\n",
    "# Función para combinar oraciones\n",
    "def combine_sentences(sentences: List[Dict[str, str]], buffer_size: int = 1) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Combina oraciones de acuerdo al tamaño del buffer definido.\n",
    "    Args:\n",
    "        sentences (List[Dict[str, str]]): Lista de oraciones con índices.\n",
    "        buffer_size (int): Número de oraciones antes y después a combinar.\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: Lista con oraciones combinadas.\n",
    "    \"\"\"\n",
    "    for i in range(len(sentences)):\n",
    "        combined_sentence = ''\n",
    "\n",
    "        # Añadir oraciones previas\n",
    "        for j in range(i - buffer_size, i):\n",
    "            if j >= 0:\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "\n",
    "        # Añadir oración actual\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "\n",
    "        # Añadir oraciones posteriores\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            if j < len(sentences):\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "\n",
    "        # Guardar la oración combinada en el dict actual\n",
    "        sentences[i]['combined_sentence'] = combined_sentence.strip()\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# Función para calcular distancias coseno\n",
    "def calculate_cosine_distances(sentences: List[Dict[str, str]], model_name: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Calcula las distancias coseno entre embeddings de oraciones combinadas.\n",
    "\n",
    "    Args:\n",
    "        sentences (List[Dict[str, Any]]): Lista de oraciones con embeddings combinados.\n",
    "        model_name (str): Nombre del modelo de embeddings.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: Distancias coseno entre embeddings consecutivos.\n",
    "    \"\"\"\n",
    "    # Crear embeddings\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    embeddings = embedding_model.embed_documents([sentence['combined_sentence'] for sentence in sentences])\n",
    "\n",
    "    # Añadir embeddings a las oraciones\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence['embedding'] = embeddings[i]\n",
    "\n",
    "    distances = list()\n",
    "    for i in range(len(sentences) - 1):\n",
    "        sim = cosine_similarity([sentences[i]['embedding']], [sentences[i + 1]['embedding']])[0][0]\n",
    "        distances.append(1 - sim)\n",
    "\n",
    "    return distances\n",
    "\n",
    "# Función para dividir en fragmentos\n",
    "def split_into_chunks(sentences: List[Dict[str, str]], distances: List[float], threshold: float) -> List[str]:\n",
    "    \"\"\"\n",
    "    Divide el texto en fragmentos basado en la distancia coseno entre oraciones.\n",
    "\n",
    "    Args:\n",
    "        sentences (List[Dict[str, str]]): Lista de oraciones.\n",
    "        distances (List[float]): Distancias entre oraciones consecutivas.\n",
    "        threshold (float): Umbral para decidir la separación de fragmentos.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Lista de fragmentos de texto.\n",
    "    \"\"\"\n",
    "    chunks = list()\n",
    "    start_index = 0\n",
    "\n",
    "    for i, distance in enumerate(distances):\n",
    "        if distance > threshold:\n",
    "            chunk = ' '.join(sentence['sentence'] for sentence in sentences[start_index:i + 1])\n",
    "            chunks.append(chunk)\n",
    "            start_index = i + 1\n",
    "\n",
    "    if start_index < len(sentences):\n",
    "        chunk = ' '.join(sentence['sentence'] for sentence in sentences[start_index:])\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def extract_metadata(text_chunk: str) -> Tuple[Optional[str], Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Extrae títulos, subtítulos y sub-subtítulos de un fragmento de texto.\n",
    "\n",
    "    Args:\n",
    "        text_chunk (str): Fragmento de texto del cual extraer la metadata.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Optional[str], Optional[str], Optional[str]]: Título, subtítulo y sub-subtítulo encontrados (o None si no se encuentran).\n",
    "    \"\"\"\n",
    "    title_pattern = re.compile(r\"PART \\d+[-—]\\s*[A-Za-z0-9 ,.\\-]+\")\n",
    "    subtitle_pattern = re.compile(r\"Subpart [A-Z]—[A-Za-z0-9 ,\\\\-]+\")\n",
    "    sub_subtitle_pattern = re.compile(r\"§\\s*\\d+\\.\\d+\\s+[A-Za-z0-9 ,.\\-]+\")\n",
    "\n",
    "    title = title_pattern.search(text_chunk)\n",
    "    subtitle = subtitle_pattern.search(text_chunk)\n",
    "    sub_subtitle = sub_subtitle_pattern.search(text_chunk)\n",
    "\n",
    "    return (\n",
    "        title.group(0).strip() if title else None,\n",
    "        subtitle.group(0).strip() if subtitle else None,\n",
    "        sub_subtitle.group(0).strip() if sub_subtitle else None,\n",
    "    )\n",
    "\n",
    "def assign_metadata_to_chunks_with_context(chunks: List[str], max_previous_chunks: int = 100) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Asigna títulos, subtítulos y sub-subtítulos como metadata a cada chunk.\n",
    "\n",
    "    Args:\n",
    "        chunks (List[str]): Lista de fragmentos de texto.\n",
    "        max_previous_chunks (int): Número máximo de fragmentos previos a considerar para acumular metadata.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: Lista de fragmentos con metadata asignada.\n",
    "    \"\"\"\n",
    "    annotated_chunks = list()\n",
    "    for i in range(len(chunks)):\n",
    "        metadata_accumulated = {\"title\": None, \"subtitle\": None, \"sub_subtitle\": None}\n",
    "        for j in range(max(0, i - max_previous_chunks), i):\n",
    "            metadata_title, metadata_subtitle, metadata_subsubtitle = extract_metadata(chunks[j])\n",
    "            if metadata_title:\n",
    "                metadata_accumulated[\"title\"] = metadata_title\n",
    "            if metadata_subtitle:\n",
    "                metadata_accumulated[\"subtitle\"] = metadata_subtitle\n",
    "            if metadata_subsubtitle:\n",
    "                metadata_accumulated[\"sub_subtitle\"] = metadata_subsubtitle\n",
    "        annotated_chunks.append({\"chunk_text\": chunks[i], \"metadata\": metadata_accumulated.copy()})\n",
    "    return annotated_chunks\n",
    "\n",
    "# --- Qdrant Vector Store ---\n",
    "def create_qdrant_store(model_name: str, chunks: List[str]) -> QdrantVectorStore:\n",
    "    \"\"\"\n",
    "    Crea y devuelve un QdrantVectorStore a partir de un modelo de embeddings y una lista de chunks de texto.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Nombre del modelo de embeddings.\n",
    "        chunks (List[str]): Lista de fragmentos de texto.\n",
    "\n",
    "    Returns:\n",
    "        QdrantVectorStore: Objeto de almacenamiento Qdrant.\n",
    "    \"\"\"\n",
    "    # Crear embeddings con el modelo especificado\n",
    "    open_source_embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "    # Preparar documentos para Qdrant\n",
    "    documents_for_qdrant = [\n",
    "        Document(\n",
    "            page_content=item['chunk_text'], \n",
    "            metadata={\n",
    "                \"title\": item['metadata'].get('title', ''),\n",
    "                \"subtitle\": item['metadata'].get('subtitle', ''),\n",
    "                \"sub_subtitle\": item['metadata'].get('sub_subtitle', '')\n",
    "            }\n",
    "        )\n",
    "        for i, item in enumerate(chunks)  # Aquí se itera sobre los datos originales con metadatos\n",
    "    ]\n",
    "\n",
    "    # Crear la tienda de vectores en memoria\n",
    "    qdrant = QdrantVectorStore.from_documents(\n",
    "        documents_for_qdrant,\n",
    "        embedding=open_source_embeddings,\n",
    "        sparse_embedding=sparse_embeddings,\n",
    "        location=\":memory:\",  # Puedes cambiar la ubicación para persistencia\n",
    "        collection_name=\"my_documents\",\n",
    "        retrieval_mode=RetrievalMode.HYBRID,\n",
    "    )\n",
    "    \n",
    "    return qdrant \n",
    "\n",
    "def create_llm(model_name: str, temperature: float, openai_api_key: str) -> ChatOpenAI:\n",
    "    \"\"\"\n",
    "    Crea un modelo LLM utilizando los parámetros proporcionados.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Nombre del modelo a utilizar.\n",
    "        temperature (float): Grado de creatividad en las respuestas.\n",
    "        openai_api_key (str): Clave de API de OpenAI para la autenticación.\n",
    "\n",
    "    Returns:\n",
    "        ChatOpenAI: Una instancia del modelo configurado.\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(\n",
    "        model=model_name,\n",
    "        temperature=temperature,  # Ajusta la creatividad según sea necesario\n",
    "        openai_api_key=openai_api_key\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "def create_rag_chain(qdrant: QdrantVectorStore, llm: ChatOpenAI) -> QdrantVectorStore:\n",
    "    \"\"\"\n",
    "    Crea y devuelve una cadena RAG (Retrieval-Augmented Generation) utilizando LangChain.\n",
    "\n",
    "    Args:\n",
    "        model (str): Nombre del modelo OpenAI para la generación de texto.\n",
    "        openai_api_key (str): Clave de acceso a la API de OpenAI.\n",
    "        qdrant (QdrantVectorStore): Almacén de vectores configurado para recuperar documentos relevantes.\n",
    "        temperature (float): Nivel de creatividad del modelo en la generación de texto. \n",
    "\n",
    "    Returns:\n",
    "        rag_chain: La cadena RAG configurada para generación y recuperación.\n",
    "        retriever: El objeto retriever configurado para recuperar documentos relevantes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Descargar y configurar el prompt desde LangChain Hub\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # Función para formatear los documentos\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Configurar el retriever desde Qdrant\n",
    "    retriever = qdrant.as_retriever()\n",
    "\n",
    "    # Crear la cadena RAG\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return rag_chain,retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Carga un archivo PDF y devuelve las primeras páginas como documentos.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Ruta al archivo PDF.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de documentos extraídos del PDF.\n",
    "    \"\"\"\n",
    "    # Cargar variables de entorno si son necesarias\n",
    "    load_dotenv()\n",
    "\n",
    "    # Cargar el archivo PDF\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    return docs  # Devuelve todas las páginas como lista\n",
    "\n",
    "def split_pdf_documents(docs, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Divide un documento PDF en fragmentos de texto.\n",
    "\n",
    "    Args:\n",
    "        docs (list): Lista de documentos cargados desde un PDF.\n",
    "        chunk_size (int): Tamaño de cada fragmento de texto en caracteres. Default es 1000.\n",
    "        chunk_overlap (int): Cantidad de solapamiento entre fragmentos. Default es 200.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de fragmentos de texto extraídos del PDF.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    return splits  # Devuelve los fragmentos divididos\n",
    "\n",
    "# --- Qdrant Vector Store ---\n",
    "def create_qdrant_store_naive(model_name: str, chunks: List[str]) -> QdrantVectorStore:\n",
    "    \"\"\"\n",
    "    Crea y devuelve un QdrantVectorStore a partir de un modelo de embeddings y una lista de chunks de texto.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Nombre del modelo de embeddings.\n",
    "        chunks (List[str]): Lista de fragmentos de texto.\n",
    "\n",
    "    Returns:\n",
    "        QdrantVectorStore: Objeto de almacenamiento Qdrant.\n",
    "    \"\"\"\n",
    "    # Crear embeddings con el modelo especificado\n",
    "    open_source_embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    client = QdrantClient(path=\"/tmp/langchain_qdrant5\")\n",
    "\n",
    "    try:\n",
    "        client.get_collection(\"naive_documents\")\n",
    "    except ValueError:\n",
    "        client.create_collection(\n",
    "            collection_name=\"naive_documents\",\n",
    "            vectors_config=VectorParams(size=384,\n",
    "                                        distance=Distance.COSINE),\n",
    "        )\n",
    "\n",
    "    qdrant = QdrantVectorStore(\n",
    "        client=client,\n",
    "        collection_name=\"naive_documents\",\n",
    "        embedding=open_source_embeddings\n",
    "    )\n",
    "    uuids = [str(uuid4()) for _ in range(len(chunks))]\n",
    "    qdrant.add_documents(documents=chunks, ids=uuids)\n",
    "    client.close()\n",
    "    return qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_factoid_qa_prompt():\n",
    "    \"\"\"\n",
    "    Genera un prompt mejorado para la creación de preguntas y respuestas factuales basado en un contexto.\n",
    "\n",
    "    Returns:\n",
    "        ChatPromptTemplate: Un objeto de plantilla de prompt para generación de QA.\n",
    "    \"\"\"\n",
    "    QA_generation_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Your task is to generate a *factoid question* and its corresponding *answer* based on the given context.\n",
    "\n",
    "    Here are the rules:\n",
    "    1. The *factoid question* must be directly answerable with a specific and concise piece of factual information from the context.\n",
    "    2. Avoid using phrases like \"according to the passage\" or \"based on the context\" in your question.\n",
    "    3. The question should resemble the style of queries typically entered in a search engine, focusing on clarity and relevance.\n",
    "    4. The context provided will have a maximum token limit of 200 to 300 tokens.                                                        \n",
    "\n",
    "    Please provide your response in the following format:\n",
    "\n",
    "    Output:::\n",
    "    Factoid question: (Your factoid question here)\n",
    "    Answer: (The answer to the factoid question here)\n",
    "\n",
    "    Here is the context:\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Output:::\n",
    "    \"\"\")\n",
    "    return QA_generation_prompt\n",
    "\n",
    "\n",
    "# Corregir la implementación del Passthrough\n",
    "class SimplePassthrough:\n",
    "    def __call__(self, inputs):\n",
    "        return inputs  # Devuelve los inputs directamente\n",
    "\n",
    "\n",
    "# Definimos una función para manejar un solo contexto\n",
    "def question_chain(context, prompt, config):\n",
    "    passthrough = SimplePassthrough()  # Instanciar el passthrough\n",
    "    passthrough_output = passthrough({\"context\": context})  # Paso directo\n",
    "\n",
    "    # Formatear el prompt\n",
    "    prompt_output = prompt.format(**passthrough_output)\n",
    "    \n",
    "    llm = create_llm(config[\"model\"], config[\"temperature\"], config[\"openai_api_key\"])\n",
    "\n",
    "    # Generar salida usando el modelo LLM\n",
    "    llm_output = llm.invoke(prompt_output)\n",
    "    \n",
    "    # Parsear la salida final\n",
    "    parsed_output = StrOutputParser().parse(llm_output)\n",
    "    \n",
    "    return parsed_output\n",
    "\n",
    "# Proceso para manejar varios documentos\n",
    "def process_multiple_docs(docs, prompt, config, num_samples=15):\n",
    "    # Seleccionar una muestra aleatoria de documentos\n",
    "    sampled_docs = random.sample(docs, num_samples)\n",
    "    sampled_docs_processed = [doc.page_content for doc in sampled_docs]\n",
    "    \n",
    "    # Procesar preguntas en batch\n",
    "    questions = [\n",
    "        question_chain(sampled_context, prompt, config)\n",
    "        for sampled_context in tqdm(sampled_docs_processed, desc=\"Processing questions\")\n",
    "    ]\n",
    "    \n",
    "    return questions\n",
    "\n",
    "def extract_questions_and_answers(data):\n",
    "    questions = list()\n",
    "    answers = list()\n",
    "    for message in data:\n",
    "        if hasattr(message, \"content\"):\n",
    "            content = message.content\n",
    "            if \"Factoid question:\" in content and \"Answer:\" in content:\n",
    "                # Extraer pregunta y respuesta\n",
    "                factoid_question = content.split(\"Factoid question:\")[1].split(\"\\nAnswer:\")[0].strip()\n",
    "                answer = content.split(\"Answer:\")[1].strip()\n",
    "                # Agregar a las listas correspondientes\n",
    "                questions.append(factoid_question)\n",
    "                answers.append(answer)\n",
    "    return questions, answers\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_rag_pipeline(rag_chain, retriever, questions, ground_truths):\n",
    "    \"\"\"\n",
    "    Realiza la inferencia con un pipeline RAG, evalúa los resultados y devuelve un DataFrame con las métricas.\n",
    "\n",
    "    Args:\n",
    "        rag_chain: El modelo RAG para generar respuestas.\n",
    "        retriever: El componente de recuperación para obtener contextos relevantes.\n",
    "        questions (list): Lista de preguntas para realizar la inferencia.\n",
    "        ground_truths (list): Lista de respuestas esperadas (ground truths) para evaluación.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame con los resultados de la evaluación.\n",
    "    \"\"\"\n",
    "    # Inicializar listas para almacenar respuestas y contextos\n",
    "    answers = list()\n",
    "    contexts = list()\n",
    "\n",
    "    # Inferencia para cada pregunta\n",
    "    for query in questions:\n",
    "        # Obtener respuesta del modelo\n",
    "        answers.append(rag_chain.invoke(query))\n",
    "        # Obtener contextos relevantes del retriever\n",
    "        relevant_docs = retriever.invoke(query)\n",
    "        contexts.append([doc.page_content for doc in relevant_docs])\n",
    "\n",
    "    # Crear conjunto de datos para evaluación\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truths\": ground_truths,\n",
    "        \"reference\": [gt[0] for gt in ground_truths]  # Agregar columna de referencia\n",
    "    })\n",
    "\n",
    "    # Evaluar el pipeline utilizando métricas de RAGAs\n",
    "    result = evaluate(\n",
    "        dataset=dataset, \n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Convertir resultados a DataFrame y devolver\n",
    "    df = result.to_pandas()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Cargando archivo: c:\\Users\\jomunozf\\Documents\\GitHub\\practicos-rag\\data\\CFR-2024-vol8.pdf\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/paraphrase-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/paraphrase-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a28c4b1d5b8415fb85655e08e7b9c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 29 files:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:   0%|          | 0/15 [00:00<?, ?it/s]C:\\Users\\jomunozf\\AppData\\Local\\Temp\\ipykernel_28752\\2023844394.py:49: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  llm_output = llm(prompt_output)\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:   7%|▋         | 1/15 [00:01<00:20,  1.47s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  13%|█▎        | 2/15 [00:02<00:16,  1.24s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  20%|██        | 3/15 [00:03<00:13,  1.09s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  27%|██▋       | 4/15 [00:04<00:13,  1.23s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  33%|███▎      | 5/15 [00:06<00:13,  1.30s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  40%|████      | 6/15 [00:07<00:10,  1.14s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  47%|████▋     | 7/15 [00:07<00:08,  1.04s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  53%|█████▎    | 8/15 [00:09<00:07,  1.07s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  60%|██████    | 9/15 [00:10<00:06,  1.01s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  67%|██████▋   | 10/15 [00:10<00:04,  1.04it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  73%|███████▎  | 11/15 [00:11<00:03,  1.02it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  80%|████████  | 12/15 [00:12<00:02,  1.02it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  87%|████████▋ | 13/15 [00:13<00:01,  1.02it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  93%|█████████▎| 14/15 [00:14<00:00,  1.04it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions: 100%|██████████| 15/15 [00:15<00:00,  1.06s/it]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a750f7a937a54561b61752a0eb47cd99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Respuesta: The procedures for milk pasteurization according to the regulations mentioned in the document involve heating every particle of milk or cream to a specific temperature and holding it there continuously for a specified time. The temperatures given in the document are 145 °F (63 °C) for 30 minutes or at least 161 °F for at least 15 seconds. These procedures are mandatory for all milk and milk products intended for direct human consumption.\n"
     ]
    }
   ],
   "source": [
    "# --- Flujo Principal ---\n",
    "def main():\n",
    "    os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "    load_dotenv()\n",
    "    config = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"openai_api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "        \"model_name\": \"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n",
    "        \"directory_path\": os.getcwd()+ \"\\\\data\\\\\",\n",
    "        \"buffer_size\": 2,\n",
    "        \"threshold\": 0.3,\n",
    "        \"max_previous_chunks\": 400,\n",
    "        \"temperature\": 0.7,\n",
    "        \"file_path\":  os.getcwd()+ \"\\\\data\\\\\" + \"CFR-2024-vol8.pdf\",\n",
    "        \"num_samples\":15,\n",
    "    }\n",
    "    #SuperRAG\n",
    "    pdf_texts = load_pdf_all_documents(config[\"directory_path\"])\n",
    "    cleaned_text = clean_text_and_exclude_sections(\" \".join(pdf_texts))\n",
    "    sentences = split_text_into_sentences(cleaned_text)\n",
    "    combined_sentences = combine_sentences(sentences, config[\"buffer_size\"])\n",
    "    distances = calculate_cosine_distances(combined_sentences, config[\"model_name\"])\n",
    "    chunks = split_into_chunks(combined_sentences, distances, config[\"threshold\"])\n",
    "    annotated_chunks = assign_metadata_to_chunks_with_context(chunks, config[\"max_previous_chunks\"])\n",
    "    qdrant_store = create_qdrant_store(config[\"model_name\"], annotated_chunks)\n",
    "    llm = create_llm(config[\"model\"], config[\"temperature\"], config[\"openai_api_key\"])\n",
    "    rag_chain, retriever = create_rag_chain(qdrant_store,llm)\n",
    "\n",
    "    #Raga\n",
    "    loader = PyPDFLoader(config[\"file_path\"])\n",
    "    docs = loader.load()\n",
    "    prompt = generate_factoid_qa_prompt()\n",
    "    new_questions = process_multiple_docs(docs, prompt, config, config[\"num_samples\"])\n",
    "    questions, ground_truths = extract_questions_and_answers(new_questions)\n",
    "    df_raga = evaluate_rag_pipeline(rag_chain, retriever, questions, ground_truths)\n",
    "    df_raga.to_csv(\"results.csv\", encoding = \"utf-8\", sep = \"|\")\n",
    "\n",
    "\n",
    "    question = \"What are the procedures for milk pasteurization according to the regulations mentioned in the document?\"\n",
    "    response = rag_chain.invoke(question)\n",
    "    logging.info(f\"Respuesta: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"openai_api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "    \"model_name\": \"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n",
    "    \"directory_path\": os.getcwd()+ \"\\\\data\\\\\",\n",
    "    \"buffer_size\": 2,\n",
    "    \"threshold\": 0.3,\n",
    "    \"max_previous_chunks\": 400,\n",
    "    \"temperature\": 0.7,\n",
    "    \"file_path\":  os.getcwd()+ \"\\\\data\\\\\" + \"CFR-2024-vol8.pdf\",\n",
    "    \"num_samples\":15,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "docs = load_pdf(config[\"file_path\"])\n",
    "naive_chunks = split_pdf_documents(docs)\n",
    "naive_qdrant = create_qdrant_store_naive(config[\"model_name\"], naive_chunks)\n",
    "llm = create_llm(config[\"model\"], config[\"temperature\"], config[\"openai_api_key\"])\n",
    "naive_rag_chain, naive_retriever = create_rag_chain(naive_qdrant, llm )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the regulations mentioned in the document, the procedures for milk pasteurization are as follows:\\n\\n1. Milk must be heated to a minimum temperature of 161°F (71.7°C) for at least 15 seconds for batch pasteurization or to a minimum temperature of 145°F (62.8°C) for at least 30 minutes for vat pasteurization.\\n\\n2. The milk must be cooled rapidly to a temperature of 45°F (7.2°C) or lower after pasteurization to prevent the growth of harmful bacteria.\\n\\n3. Proper sanitation procedures must be followed to prevent contamination of the milk during pasteurization.\\n\\n4. Records of pasteurization times, temperatures, and cooling procedures must be maintained for inspection by regulatory authorities.\\n\\n5. Pasteurized milk must be stored and transported in clean, sanitized containers to prevent contamination.\\n\\n6. Pasteurized milk must be labeled as such to indicate that it has been properly pasteurized according to regulations.\\n\\n7. Any deviations from the pasteurization procedures must be documented and corrective actions taken to prevent future occurrences.\\n\\nOverall, the goal of milk pasteurization is to ensure the safety and quality of the milk by destroying harmful bacteria while preserving its nutritional value.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_rag_chain.invoke(\"What are the procedures for milk pasteurization according to the regulations mentioned in the document?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedures for milk pasteurization according to the regulations mentioned in the document involve heating every particle of milk or cream to a specific temperature and holding it there continuously for a specified time. The temperatures given in the document are 145 °F (63 °C) for 30 minutes or at least 161 °F for at least 15 seconds. These procedures are mandatory for all milk and milk products intended for direct human consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'According to the regulations mentioned in the document, the procedures for milk pasteurization are as follows:\\n\\n1. Milk must be heated to a minimum temperature of 161°F (71.7°C) for at least 15 seconds for batch pasteurization or to a minimum temperature of 145°F (62.8°C) for at least 30 minutes for vat pasteurization.\\n\\n2. The milk must be cooled rapidly to a temperature of 45°F (7.2°C) or lower after pasteurization to prevent the growth of harmful bacteria.\\n\\n3. Proper sanitation procedures must be followed to prevent contamination of the milk during pasteurization.\\n\\n4. Records of pasteurization times, temperatures, and cooling procedures must be maintained for inspection by regulatory authorities.\\n\\n5. Pasteurized milk must be stored and transported in clean, sanitized containers to prevent contamination.\\n\\n6. Pasteurized milk must be labeled as such to indicate that it has been properly pasteurized according to regulations.\\n\\n7. Any deviations from the pasteurization procedures must be documented and corrective actions taken to prevent future occurrences.\\n\\nOverall, the goal of milk pasteurization is to ensure the safety and quality of the milk by destroying harmful bacteria while preserving its nutritional value.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# pd.read_csv(f'{os.getcwd()}\\\\results.csv', sep = '|').to_excel('results.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entorno_prueba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
