{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flujo completo RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/v0a02bg/practicos-rag/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from dotenv import load_dotenv\n",
    "from datasets import Dataset\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_qdrant import FastEmbedSparse, RetrievalMode\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "#from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from qdrant_client import QdrantClient\n",
    "from uuid import uuid4\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configurar logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# --- PDF Processing ---\n",
    "def load_pdf_all_documents(directory_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Carga documentos PDF desde una carpeta y devuelve una lista de páginas como texto.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Ruta de la carpeta que contiene los archivos PDF.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Lista de cadenas de texto, donde cada cadena corresponde al texto extraído de una página PDF.\n",
    "    \"\"\"\n",
    "    from PyPDF2 import PdfReader\n",
    "\n",
    "    all_texts = list()\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            try:\n",
    "                logging.info(f\"Cargando archivo: {file_path}\")\n",
    "                reader = PdfReader(file_path)\n",
    "                for page in reader.pages:\n",
    "                    all_texts.append(page.extract_text())\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error leyendo el archivo PDF: {file_path}. Detalle: {e}\")\n",
    "    return all_texts\n",
    "\n",
    "# --- Text Processing ---\n",
    "def clean_text_and_exclude_sections(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpia el texto eliminando espacios redundantes y caracteres especiales.\n",
    "\n",
    "    Args:\n",
    "        text (str): Texto a limpiar.\n",
    "\n",
    "    Returns:\n",
    "        str: Texto limpio con espacios redundantes eliminados.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Reemplazar múltiples espacios\n",
    "    return text.strip()\n",
    "\n",
    "# Función para dividir texto en oraciones\n",
    "def split_text_into_sentences(text: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Divide un texto en oraciones basado en '.', '?', y '!' y devuelve una lista de diccionarios.\n",
    "    Args:\n",
    "        text (str): El texto a dividir.\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: Lista de diccionarios con 'sentence' y 'index'.\n",
    "    \"\"\"\n",
    "    single_sentences_list = re.split(r'(?<=[.?!])\\s+', text.strip())\n",
    "    sentences = [{'sentence': sentence, 'index': i} for i, sentence in enumerate(single_sentences_list)]\n",
    "    return sentences\n",
    "\n",
    "# Función para combinar oraciones\n",
    "def combine_sentences(sentences: List[Dict[str, str]], buffer_size: int = 1) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Combina oraciones de acuerdo al tamaño del buffer definido.\n",
    "    Args:\n",
    "        sentences (List[Dict[str, str]]): Lista de oraciones con índices.\n",
    "        buffer_size (int): Número de oraciones antes y después a combinar.\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: Lista con oraciones combinadas.\n",
    "    \"\"\"\n",
    "    for i in range(len(sentences)):\n",
    "        combined_sentence = ''\n",
    "\n",
    "        # Añadir oraciones previas\n",
    "        for j in range(i - buffer_size, i):\n",
    "            if j >= 0:\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "\n",
    "        # Añadir oración actual\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "\n",
    "        # Añadir oraciones posteriores\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            if j < len(sentences):\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "\n",
    "        # Guardar la oración combinada en el dict actual\n",
    "        sentences[i]['combined_sentence'] = combined_sentence.strip()\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# Función para calcular distancias coseno\n",
    "def calculate_cosine_distances(sentences: List[Dict[str, str]], model_name: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Calcula las distancias coseno entre embeddings de oraciones combinadas.\n",
    "\n",
    "    Args:\n",
    "        sentences (List[Dict[str, Any]]): Lista de oraciones con embeddings combinados.\n",
    "        model_name (str): Nombre del modelo de embeddings.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: Distancias coseno entre embeddings consecutivos.\n",
    "    \"\"\"\n",
    "    # Crear embeddings\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    embeddings = embedding_model.embed_documents([sentence['combined_sentence'] for sentence in sentences])\n",
    "\n",
    "    # Añadir embeddings a las oraciones\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence['embedding'] = embeddings[i]\n",
    "\n",
    "    distances = list()\n",
    "    for i in range(len(sentences) - 1):\n",
    "        sim = cosine_similarity([sentences[i]['embedding']], [sentences[i + 1]['embedding']])[0][0]\n",
    "        distances.append(1 - sim)\n",
    "\n",
    "    return distances\n",
    "\n",
    "# Función para dividir en fragmentos\n",
    "def split_into_chunks(sentences: List[Dict[str, str]], distances: List[float], threshold: float) -> List[str]:\n",
    "    \"\"\"\n",
    "    Divide el texto en fragmentos basado en la distancia coseno entre oraciones.\n",
    "\n",
    "    Args:\n",
    "        sentences (List[Dict[str, str]]): Lista de oraciones.\n",
    "        distances (List[float]): Distancias entre oraciones consecutivas.\n",
    "        threshold (float): Umbral para decidir la separación de fragmentos.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Lista de fragmentos de texto.\n",
    "    \"\"\"\n",
    "    chunks = list()\n",
    "    start_index = 0\n",
    "\n",
    "    for i, distance in enumerate(distances):\n",
    "        if distance > threshold:\n",
    "            chunk = ' '.join(sentence['sentence'] for sentence in sentences[start_index:i + 1])\n",
    "            chunks.append(chunk)\n",
    "            start_index = i + 1\n",
    "\n",
    "    if start_index < len(sentences):\n",
    "        chunk = ' '.join(sentence['sentence'] for sentence in sentences[start_index:])\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def extract_metadata(text_chunk: str) -> Tuple[Optional[str], Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Extrae títulos, subtítulos y sub-subtítulos de un fragmento de texto.\n",
    "\n",
    "    Args:\n",
    "        text_chunk (str): Fragmento de texto del cual extraer la metadata.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Optional[str], Optional[str], Optional[str]]: Título, subtítulo y sub-subtítulo encontrados (o None si no se encuentran).\n",
    "    \"\"\"\n",
    "    title_pattern = re.compile(r\"PART \\d+[-—]\\s*[A-Za-z0-9 ,.\\-]+\")\n",
    "    subtitle_pattern = re.compile(r\"Subpart [A-Z]—[A-Za-z0-9 ,\\\\-]+\")\n",
    "    sub_subtitle_pattern = re.compile(r\"§\\s*\\d+\\.\\d+\\s+[A-Za-z0-9 ,.\\-]+\")\n",
    "\n",
    "    title = title_pattern.search(text_chunk)\n",
    "    subtitle = subtitle_pattern.search(text_chunk)\n",
    "    sub_subtitle = sub_subtitle_pattern.search(text_chunk)\n",
    "\n",
    "    return (\n",
    "        title.group(0).strip() if title else None,\n",
    "        subtitle.group(0).strip() if subtitle else None,\n",
    "        sub_subtitle.group(0).strip() if sub_subtitle else None,\n",
    "    )\n",
    "\n",
    "def assign_metadata_to_chunks_with_context(chunks: List[str], max_previous_chunks: int = 100) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Asigna títulos, subtítulos y sub-subtítulos como metadata a cada chunk.\n",
    "\n",
    "    Args:\n",
    "        chunks (List[str]): Lista de fragmentos de texto.\n",
    "        max_previous_chunks (int): Número máximo de fragmentos previos a considerar para acumular metadata.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: Lista de fragmentos con metadata asignada.\n",
    "    \"\"\"\n",
    "    annotated_chunks = list()\n",
    "    for i in range(len(chunks)):\n",
    "        metadata_accumulated = {\"title\": None, \"subtitle\": None, \"sub_subtitle\": None}\n",
    "        for j in range(max(0, i - max_previous_chunks), i):\n",
    "            metadata_title, metadata_subtitle, metadata_subsubtitle = extract_metadata(chunks[j])\n",
    "            if metadata_title:\n",
    "                metadata_accumulated[\"title\"] = metadata_title\n",
    "            if metadata_subtitle:\n",
    "                metadata_accumulated[\"subtitle\"] = metadata_subtitle\n",
    "            if metadata_subsubtitle:\n",
    "                metadata_accumulated[\"sub_subtitle\"] = metadata_subsubtitle\n",
    "        annotated_chunks.append({\"chunk_text\": chunks[i], \"metadata\": metadata_accumulated.copy()})\n",
    "    return annotated_chunks\n",
    "\n",
    "# --- Qdrant Vector Store ---\n",
    "def create_qdrant_store(model_name: str, chunks: List[str]) -> QdrantVectorStore:\n",
    "    \"\"\"\n",
    "    Crea y devuelve un QdrantVectorStore a partir de un modelo de embeddings y una lista de chunks de texto.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Nombre del modelo de embeddings.\n",
    "        chunks (List[str]): Lista de fragmentos de texto.\n",
    "\n",
    "    Returns:\n",
    "        QdrantVectorStore: Objeto de almacenamiento Qdrant.\n",
    "    \"\"\"\n",
    "    # Crear embeddings con el modelo especificado\n",
    "    open_source_embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "    # Preparar documentos para Qdrant\n",
    "    documents_for_qdrant = [\n",
    "        Document(\n",
    "            page_content=item['chunk_text'], \n",
    "            metadata={\n",
    "                \"title\": item['metadata'].get('title', ''),\n",
    "                \"subtitle\": item['metadata'].get('subtitle', ''),\n",
    "                \"sub_subtitle\": item['metadata'].get('sub_subtitle', '')\n",
    "            }\n",
    "        )\n",
    "        for i, item in enumerate(chunks)  # Aquí se itera sobre los datos originales con metadatos\n",
    "    ]\n",
    "\n",
    "    # Crear la tienda de vectores en memoria\n",
    "    qdrant = QdrantVectorStore.from_documents(\n",
    "        documents_for_qdrant,\n",
    "        embedding=open_source_embeddings,\n",
    "        sparse_embedding=sparse_embeddings,\n",
    "        location=\":memory:\",  # Puedes cambiar la ubicación para persistencia\n",
    "        collection_name=\"my_documents\",\n",
    "        retrieval_mode=RetrievalMode.HYBRID,\n",
    "    )\n",
    "    \n",
    "    return qdrant \n",
    "\n",
    "def create_llm(model_name: str, temperature: float, openai_api_key: str) -> ChatOpenAI:\n",
    "    \"\"\"\n",
    "    Crea un modelo LLM utilizando los parámetros proporcionados.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Nombre del modelo a utilizar.\n",
    "        temperature (float): Grado de creatividad en las respuestas.\n",
    "        openai_api_key (str): Clave de API de OpenAI para la autenticación.\n",
    "\n",
    "    Returns:\n",
    "        ChatOpenAI: Una instancia del modelo configurado.\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(\n",
    "        model=model_name,\n",
    "        temperature=temperature,  # Ajusta la creatividad según sea necesario\n",
    "        openai_api_key=openai_api_key\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "def create_rag_chain(qdrant: QdrantVectorStore, llm: ChatOpenAI) -> QdrantVectorStore:\n",
    "    \"\"\"\n",
    "    Crea y devuelve una cadena RAG (Retrieval-Augmented Generation) utilizando LangChain.\n",
    "\n",
    "    Args:\n",
    "        model (str): Nombre del modelo OpenAI para la generación de texto.\n",
    "        openai_api_key (str): Clave de acceso a la API de OpenAI.\n",
    "        qdrant (QdrantVectorStore): Almacén de vectores configurado para recuperar documentos relevantes.\n",
    "        temperature (float): Nivel de creatividad del modelo en la generación de texto. \n",
    "\n",
    "    Returns:\n",
    "        rag_chain: La cadena RAG configurada para generación y recuperación.\n",
    "        retriever: El objeto retriever configurado para recuperar documentos relevantes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Descargar y configurar el prompt desde LangChain Hub\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # Función para formatear los documentos\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Configurar el retriever desde Qdrant\n",
    "    retriever = qdrant.as_retriever()\n",
    "\n",
    "    # Crear la cadena RAG\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return rag_chain,retriever\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Carga un archivo PDF y devuelve las primeras páginas como documentos.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Ruta al archivo PDF.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de documentos extraídos del PDF.\n",
    "    \"\"\"\n",
    "    # Cargar variables de entorno si son necesarias\n",
    "    load_dotenv()\n",
    "\n",
    "    # Cargar el archivo PDF\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    return docs  # Devuelve todas las páginas como lista\n",
    "\n",
    "def split_pdf_documents(docs, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Divide un documento PDF en fragmentos de texto.\n",
    "\n",
    "    Args:\n",
    "        docs (list): Lista de documentos cargados desde un PDF.\n",
    "        chunk_size (int): Tamaño de cada fragmento de texto en caracteres. Default es 1000.\n",
    "        chunk_overlap (int): Cantidad de solapamiento entre fragmentos. Default es 200.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de fragmentos de texto extraídos del PDF.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    return splits  # Devuelve los fragmentos divididos\n",
    "\n",
    "# --- Qdrant Vector Store ---\n",
    "def create_qdrant_store_naive(model_name: str, chunks: List[str]) -> QdrantVectorStore:\n",
    "    \"\"\"\n",
    "    Crea y devuelve un QdrantVectorStore a partir de un modelo de embeddings y una lista de chunks de texto.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Nombre del modelo de embeddings.\n",
    "        chunks (List[str]): Lista de fragmentos de texto.\n",
    "\n",
    "    Returns:\n",
    "        QdrantVectorStore: Objeto de almacenamiento Qdrant.\n",
    "    \"\"\"\n",
    "    # Crear embeddings con el modelo especificado\n",
    "    open_source_embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    client = QdrantClient(path=\"/tmp/langchain_qdrant5\")\n",
    "\n",
    "    try:\n",
    "        client.get_collection(\"naive_documents\")\n",
    "    except ValueError:\n",
    "        client.create_collection(\n",
    "            collection_name=\"naive_documents\",\n",
    "            vectors_config=VectorParams(size=384,\n",
    "                                        distance=Distance.COSINE),\n",
    "        )\n",
    "\n",
    "    qdrant = QdrantVectorStore(\n",
    "        client=client,\n",
    "        collection_name=\"naive_documents\",\n",
    "        embedding=open_source_embeddings\n",
    "    )\n",
    "    uuids = [str(uuid4()) for _ in range(len(chunks))]\n",
    "    qdrant.add_documents(documents=chunks, ids=uuids)\n",
    "    #client.close()\n",
    "    return qdrant\n",
    "\n",
    "def generate_factoid_qa_prompt():\n",
    "    \"\"\"\n",
    "    Genera un prompt mejorado para la creación de preguntas y respuestas factuales basado en un contexto.\n",
    "\n",
    "    Returns:\n",
    "        ChatPromptTemplate: Un objeto de plantilla de prompt para generación de QA.\n",
    "    \"\"\"\n",
    "    QA_generation_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Your task is to generate a *factoid question* and its corresponding *answer* based on the given context.\n",
    "\n",
    "    Here are the rules:\n",
    "    1. The *factoid question* must be directly answerable with a specific and concise piece of factual information from the context.\n",
    "    2. Avoid using phrases like \"according to the passage\" or \"based on the context\" in your question.\n",
    "    3. The question should resemble the style of queries typically entered in a search engine, focusing on clarity and relevance.\n",
    "    4. The context provided will have a maximum token limit of 200 to 300 tokens.                                                        \n",
    "\n",
    "    Please provide your response in the following format:\n",
    "\n",
    "    Output:::\n",
    "    Factoid question: (Your factoid question here)\n",
    "    Answer: (The answer to the factoid question here)\n",
    "\n",
    "    Here is the context:\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Output:::\n",
    "    \"\"\")\n",
    "    return QA_generation_prompt\n",
    "\n",
    "\n",
    "def question_chain(context: str, prompt: str, config: dict) -> str:\n",
    "    \"\"\"\n",
    "    Genera una respuesta basada en un contexto y un modelo de lenguaje configurado.\n",
    "\n",
    "    Args:\n",
    "        context (str): Texto que proporciona el contexto para la generación de la respuesta.\n",
    "        prompt (str): Plantilla del prompt que será formateada con el contexto proporcionado.\n",
    "        config (dict): Configuración para el modelo de lenguaje, incluyendo el nombre del modelo,\n",
    "                       la temperatura de generación y la clave de API de OpenAI.\n",
    "\n",
    "    Returns:\n",
    "        str: Respuesta generada por el modelo de lenguaje tras procesar el contexto y el prompt.\n",
    "    \"\"\"\n",
    "    # Clase SimplePassthrough definida dentro de la función\n",
    "    class SimplePassthrough:\n",
    "        def __call__(self, inputs: dict) -> dict:\n",
    "            return inputs  # Devuelve los inputs directamente\n",
    "\n",
    "    passthrough = SimplePassthrough()  # Instanciar el passthrough\n",
    "    passthrough_output = passthrough({\"context\": context})  # Paso directo\n",
    "\n",
    "    # Formatear el prompt\n",
    "    prompt_output = prompt.format(**passthrough_output)\n",
    "    \n",
    "    llm = create_llm(config[\"model\"], config[\"temperature\"], config[\"openai_api_key\"])\n",
    "\n",
    "    # Generar salida usando el modelo LLM\n",
    "    llm_output = llm.invoke(prompt_output)\n",
    "    \n",
    "    # Parsear la salida final\n",
    "    parsed_output = StrOutputParser().parse(llm_output)\n",
    "    \n",
    "    return parsed_output\n",
    "\n",
    "\n",
    "def process_multiple_docs(docs: List[object], prompt: str, config: Dict[str, str], num_samples: int = 15) -> List[str]:\n",
    "    \"\"\"\n",
    "    Procesa múltiples documentos seleccionando una muestra aleatoria y generando preguntas basadas en el contexto.\n",
    "\n",
    "    Args:\n",
    "        docs (List[object]): Lista de objetos de documentos, donde cada documento tiene un atributo `page_content`.\n",
    "        prompt (str): Plantilla del prompt que será utilizada para generar preguntas.\n",
    "        config (Dict[str, str]): Configuración para el modelo de lenguaje, incluyendo el nombre del modelo,\n",
    "                                 la temperatura de generación y la clave de API de OpenAI.\n",
    "        num_samples (int, opcional): Número de documentos a seleccionar aleatoriamente para procesar. \n",
    "                                     Por defecto es 15.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Lista de preguntas generadas por el modelo de lenguaje para cada documento de la muestra.\n",
    "    \"\"\"\n",
    "    # Seleccionar una muestra aleatoria de documentos\n",
    "    sampled_docs = random.sample(docs, num_samples)\n",
    "    sampled_docs_processed = [doc.page_content for doc in sampled_docs]\n",
    "    \n",
    "    # Procesar preguntas en batch\n",
    "    questions = [\n",
    "        question_chain(sampled_context, prompt, config)\n",
    "        for sampled_context in tqdm(sampled_docs_processed, desc=\"Processing questions\")\n",
    "    ]\n",
    "    \n",
    "    return questions\n",
    "\n",
    "def extract_questions_and_answers(data: List[object]) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Extrae preguntas y respuestas factuales de una lista de mensajes.\n",
    "\n",
    "    Args:\n",
    "        data (List[object]): Lista de objetos de mensajes, cada uno con un atributo `content` que contiene texto.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], List[str]]: Una tupla que contiene dos listas:\n",
    "            - La primera lista contiene las preguntas factuales extraídas.\n",
    "            - La segunda lista contiene las respuestas correspondientes.\n",
    "    \"\"\"\n",
    "    questions = list()\n",
    "    answers = list()\n",
    "    for message in data:\n",
    "        if hasattr(message, \"content\"):\n",
    "            content = message.content\n",
    "            if \"Factoid question:\" in content and \"Answer:\" in content:\n",
    "                # Extraer pregunta y respuesta\n",
    "                factoid_question = content.split(\"Factoid question:\")[1].split(\"\\nAnswer:\")[0].strip()\n",
    "                answer = content.split(\"Answer:\")[1].strip()\n",
    "                # Agregar a las listas correspondientes\n",
    "                questions.append(factoid_question)\n",
    "                answers.append(answer)\n",
    "    return questions, answers\n",
    "\n",
    "\n",
    "def evaluate_rag_pipeline(rag_chain, retriever, questions, ground_truths):\n",
    "    \"\"\"\n",
    "    Realiza la inferencia con un pipeline RAG, evalúa los resultados y devuelve un DataFrame con las métricas.\n",
    "\n",
    "    Args:\n",
    "        rag_chain: El modelo RAG para generar respuestas.\n",
    "        retriever: El componente de recuperación para obtener contextos relevantes.\n",
    "        questions (list): Lista de preguntas para realizar la inferencia.\n",
    "        ground_truths (list): Lista de respuestas esperadas (ground truths) para evaluación.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame con los resultados de la evaluación.\n",
    "    \"\"\"\n",
    "    # Inicializar listas para almacenar respuestas y contextos\n",
    "    answers = list()\n",
    "    contexts = list()\n",
    "\n",
    "    # Inferencia para cada pregunta\n",
    "    for query in questions:\n",
    "        # Obtener respuesta del modelo\n",
    "        answers.append(rag_chain.invoke(query))\n",
    "        # Obtener contextos relevantes del retriever\n",
    "        relevant_docs = retriever.invoke(query)\n",
    "        contexts.append([doc.page_content for doc in relevant_docs])\n",
    "\n",
    "    # Crear conjunto de datos para evaluación\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truths\": ground_truths,\n",
    "        \"reference\": [gt[0] for gt in ground_truths]  # Agregar columna de referencia\n",
    "    })\n",
    "\n",
    "    # Evaluar el pipeline utilizando métricas de RAGAs\n",
    "    result = evaluate(\n",
    "        dataset=dataset, \n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Convertir resultados a DataFrame y devolver\n",
    "    df = result.to_pandas()\n",
    "    return df\n",
    "\n",
    "def load_config(file_path):\n",
    "    \"\"\"\n",
    "    Carga un archivo de configuración en formato YAML y reemplaza las variables de entorno en los valores correspondientes.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Ruta al archivo YAML que contiene la configuración.\n",
    "\n",
    "    Returns:\n",
    "        dict: Diccionario con la configuración cargada. Las variables en formato ${VAR_NAME} serán reemplazadas\n",
    "              por el valor correspondiente de las variables de entorno. Si una variable no está definida,\n",
    "              se asignará el valor 'MissingEnvVar: VAR_NAME'.\n",
    "    \"\"\"\n",
    "    load_dotenv()\n",
    "    with open(file_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "        \n",
    "        # Reemplazar las variables de entorno\n",
    "        for key, value in config.items():\n",
    "            if isinstance(value, str) and value.startswith(\"${\") and value.endswith(\"}\"):\n",
    "                env_var = value[2:-1]\n",
    "                config[key] = os.getenv(env_var, f\"MissingEnvVar: {env_var}\")\n",
    "        return config\n",
    "    \n",
    "def initialize_rag(config: dict) -> object:\n",
    "    \"\"\"\n",
    "    Inicializa los componentes de RAG (Retrieval-Augmented Generation) según el tipo especificado en la configuración.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Diccionario de configuración que contiene las siguientes claves:\n",
    "            - \"rag\" (str): Tipo de RAG a inicializar, puede ser \"super\" o \"naive\".\n",
    "            - \"model_name\" (str): Nombre del modelo a utilizar.\n",
    "            - \"model\" (str): Ruta o identificador del modelo de lenguaje a usar.\n",
    "            - \"temperature\" (float): Parámetro de temperatura para el modelo de lenguaje.\n",
    "            - \"openai_api_key\" (str): Clave de API para OpenAI.\n",
    "            - \"directory_path\" (str, opcional): Ruta al directorio que contiene archivos PDF (requerido para RAG \"super\").\n",
    "            - \"buffer_size\" (int, opcional): Número de oraciones a combinar en un segmento (requerido para RAG \"super\").\n",
    "            - \"threshold\" (float, opcional): Umbral para dividir en segmentos basado en distancia coseno (requerido para RAG \"super\").\n",
    "            - \"max_previous_chunks\" (int, opcional): Número de segmentos previos a incluir como contexto (requerido para RAG \"super\").\n",
    "            - \"file_path\" (str, opcional): Ruta a un archivo PDF único (requerido para RAG \"naive\").\n",
    "\n",
    "    Returns:\n",
    "        object: Objeto de cadena RAG inicializado según la configuración especificada.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Si la clave \"rag\" en la configuración no es \"super\" o \"naive\".\n",
    "    \"\"\"\n",
    "    rag_type = config[\"rag\"]\n",
    "    model_name = config[\"model_name\"]\n",
    "    model = config[\"model\"]\n",
    "    temperature = config[\"temperature\"]\n",
    "    openai_api_key = config[\"openai_api_key\"]\n",
    "\n",
    "    if rag_type == \"super\":\n",
    "        pdf_texts = load_pdf_all_documents(config[\"directory_path\"])\n",
    "        cleaned_text = clean_text_and_exclude_sections(\" \".join(pdf_texts))\n",
    "        sentences = split_text_into_sentences(cleaned_text)\n",
    "        combined_sentences = combine_sentences(sentences, config[\"buffer_size\"])\n",
    "        distances = calculate_cosine_distances(combined_sentences, model_name)\n",
    "        chunks = split_into_chunks(combined_sentences, distances, config[\"threshold\"])\n",
    "        annotated_chunks = assign_metadata_to_chunks_with_context(chunks, config[\"max_previous_chunks\"])\n",
    "        qdrant_store = create_qdrant_store(model_name, annotated_chunks)\n",
    "        llm = create_llm(model, temperature, openai_api_key)\n",
    "        return create_rag_chain(qdrant_store, llm)\n",
    "\n",
    "    elif rag_type == \"naive\":\n",
    "        docs = load_pdf(config[\"file_path\"])\n",
    "        naive_chunks = split_pdf_documents(docs)\n",
    "        naive_qdrant = create_qdrant_store_naive(model_name, naive_chunks)\n",
    "        llm = create_llm(model, temperature, openai_api_key)\n",
    "        return create_rag_chain(naive_qdrant, llm)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"El valor de 'rag' en la configuración no es válido. Debe ser 'super' o 'naive'.\")\n",
    "\n",
    "\n",
    "def evaluate_and_save_results(rag_chain: object, retriever: object, config: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evalúa la tubería RAG (Retrieval-Augmented Generation) y guarda los resultados en un archivo CSV.\n",
    "\n",
    "    Args:\n",
    "        rag_chain (object): Cadena RAG inicializada para realizar la generación y recuperación de respuestas.\n",
    "        retriever (object): Mecanismo de recuperación utilizado para buscar información relevante.\n",
    "        config (dict): Diccionario de configuración que contiene las siguientes claves:\n",
    "            - \"file_path\" (str): Ruta al archivo PDF que se evaluará.\n",
    "            - \"num_samples\" (int): Número de preguntas de muestra que se generarán y evaluarán.\n",
    "            - \"rag\" (str): Tipo de RAG utilizado, se usará para nombrar el archivo CSV de salida.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un DataFrame con los resultados de la evaluación de la tubería RAG, que incluye métricas y comparaciones.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Si la configuración proporcionada no incluye las claves necesarias.\n",
    "    \"\"\"\n",
    "    # Cargar el documento PDF\n",
    "    loader = PyPDFLoader(config[\"file_path\"])\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Generar el prompt para preguntas y respuestas factuales\n",
    "    prompt = generate_factoid_qa_prompt()\n",
    "\n",
    "    # Procesar documentos y generar preguntas\n",
    "    new_questions = process_multiple_docs(docs, prompt, config, config[\"num_samples\"])\n",
    "    questions, ground_truths = extract_questions_and_answers(new_questions)\n",
    "\n",
    "    # Evaluar la tubería RAG\n",
    "    df_raga = evaluate_rag_pipeline(rag_chain, retriever, questions, ground_truths)\n",
    "\n",
    "    # Cambiar el nombre del archivo basado en el tipo de RAG\n",
    "    file_name = f\"results_{config['rag']}.csv\"\n",
    "    df_raga.to_csv(file_name, encoding=\"utf-8\", sep=\"|\")\n",
    "\n",
    "    # Retornar el DataFrame con los resultados\n",
    "    return df_raga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/paraphrase-MiniLM-L6-v2\n",
      "/Users/v0a02bg/practicos-rag/.venv/lib/python3.11/site-packages/langsmith/client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "Processing questions:   0%|          | 0/2 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  50%|█████     | 1/2 [00:01<00:01,  1.91s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions: 100%|██████████| 2/2 [00:04<00:00,  2.16s/it]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  12%|█▎        | 1/8 [00:06<00:45,  6.55s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  25%|██▌       | 2/8 [00:07<00:20,  3.41s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  62%|██████▎   | 5/8 [00:10<00:05,  1.69s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  75%|███████▌  | 6/8 [00:11<00:02,  1.32s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  88%|████████▊ | 7/8 [00:13<00:01,  1.59s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating: 100%|██████████| 8/8 [00:22<00:00,  2.85s/it]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Respuesta: The regulations require dairy farms and plants to use pasteurization machinery that is easily cleaned and sanitary. This machinery must be equipped with accurate time and temperature recording devices that are in good working order. The temperature during heating and holding must be recorded on thermograph charts by an official authorized for such farms and plants.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "    # Load configuration\n",
    "    config = load_config('config.yaml')\n",
    "\n",
    "    # Initialize RAG components\n",
    "    rag_chain, retriever = initialize_rag(config)\n",
    "\n",
    "    # Evaluate and save results\n",
    "    df_raga = evaluate_and_save_results(rag_chain, retriever, config)\n",
    "\n",
    "    # Example question and response\n",
    "    question = \"What are the procedures for milk pasteurization according to the regulations mentioned in the document?\"\n",
    "    response = rag_chain.invoke(question)\n",
    "    logging.info(f\"Respuesta: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
