{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flujo completo RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from dotenv import load_dotenv\n",
    "from datasets import Dataset\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_qdrant import FastEmbedSparse, RetrievalMode\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "#from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# --- PDF Processing ---\n",
    "def load_pdf_all_documents(directory_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Carga documentos PDF desde una carpeta y devuelve una lista de páginas como texto.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Ruta de la carpeta que contiene los archivos PDF.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Lista de cadenas de texto, donde cada cadena corresponde al texto extraído de una página PDF.\n",
    "    \"\"\"\n",
    "    from PyPDF2 import PdfReader\n",
    "\n",
    "    all_texts = list()\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            try:\n",
    "                logging.info(f\"Cargando archivo: {file_path}\")\n",
    "                reader = PdfReader(file_path)\n",
    "                for page in reader.pages:\n",
    "                    all_texts.append(page.extract_text())\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error leyendo el archivo PDF: {file_path}. Detalle: {e}\")\n",
    "    return all_texts\n",
    "\n",
    "# --- Text Processing ---\n",
    "def clean_text_and_exclude_sections(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpia el texto eliminando espacios redundantes y caracteres especiales.\n",
    "\n",
    "    Args:\n",
    "        text (str): Texto a limpiar.\n",
    "\n",
    "    Returns:\n",
    "        str: Texto limpio con espacios redundantes eliminados.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Reemplazar múltiples espacios\n",
    "    return text.strip()\n",
    "\n",
    "# Función para dividir texto en oraciones\n",
    "def split_text_into_sentences(text: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Divide un texto en oraciones basado en '.', '?', y '!' y devuelve una lista de diccionarios.\n",
    "    Args:\n",
    "        text (str): El texto a dividir.\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: Lista de diccionarios con 'sentence' y 'index'.\n",
    "    \"\"\"\n",
    "    single_sentences_list = re.split(r'(?<=[.?!])\\s+', text.strip())\n",
    "    sentences = [{'sentence': sentence, 'index': i} for i, sentence in enumerate(single_sentences_list)]\n",
    "    return sentences\n",
    "\n",
    "# Función para combinar oraciones\n",
    "def combine_sentences(sentences: List[Dict[str, str]], buffer_size: int = 1) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Combina oraciones de acuerdo al tamaño del buffer definido.\n",
    "    Args:\n",
    "        sentences (List[Dict[str, str]]): Lista de oraciones con índices.\n",
    "        buffer_size (int): Número de oraciones antes y después a combinar.\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: Lista con oraciones combinadas.\n",
    "    \"\"\"\n",
    "    for i in range(len(sentences)):\n",
    "        combined_sentence = ''\n",
    "\n",
    "        # Añadir oraciones previas\n",
    "        for j in range(i - buffer_size, i):\n",
    "            if j >= 0:\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "\n",
    "        # Añadir oración actual\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "\n",
    "        # Añadir oraciones posteriores\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            if j < len(sentences):\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "\n",
    "        # Guardar la oración combinada en el dict actual\n",
    "        sentences[i]['combined_sentence'] = combined_sentence.strip()\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# Función para calcular distancias coseno\n",
    "def calculate_cosine_distances(sentences: List[Dict[str, str]], model_name: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Calcula las distancias coseno entre embeddings de oraciones combinadas.\n",
    "\n",
    "    Args:\n",
    "        sentences (List[Dict[str, Any]]): Lista de oraciones con embeddings combinados.\n",
    "        model_name (str): Nombre del modelo de embeddings.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: Distancias coseno entre embeddings consecutivos.\n",
    "    \"\"\"\n",
    "    # Crear embeddings\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    embeddings = embedding_model.embed_documents([sentence['combined_sentence'] for sentence in sentences])\n",
    "\n",
    "    # Añadir embeddings a las oraciones\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence['embedding'] = embeddings[i]\n",
    "\n",
    "    distances = list()\n",
    "    for i in range(len(sentences) - 1):\n",
    "        sim = cosine_similarity([sentences[i]['embedding']], [sentences[i + 1]['embedding']])[0][0]\n",
    "        distances.append(1 - sim)\n",
    "\n",
    "    return distances\n",
    "\n",
    "# Función para dividir en fragmentos\n",
    "def split_into_chunks(sentences: List[Dict[str, str]], distances: List[float], threshold: float) -> List[str]:\n",
    "    \"\"\"\n",
    "    Divide el texto en fragmentos basado en la distancia coseno entre oraciones.\n",
    "\n",
    "    Args:\n",
    "        sentences (List[Dict[str, str]]): Lista de oraciones.\n",
    "        distances (List[float]): Distancias entre oraciones consecutivas.\n",
    "        threshold (float): Umbral para decidir la separación de fragmentos.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Lista de fragmentos de texto.\n",
    "    \"\"\"\n",
    "    chunks = list()\n",
    "    start_index = 0\n",
    "\n",
    "    for i, distance in enumerate(distances):\n",
    "        if distance > threshold:\n",
    "            chunk = ' '.join(sentence['sentence'] for sentence in sentences[start_index:i + 1])\n",
    "            chunks.append(chunk)\n",
    "            start_index = i + 1\n",
    "\n",
    "    if start_index < len(sentences):\n",
    "        chunk = ' '.join(sentence['sentence'] for sentence in sentences[start_index:])\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def extract_metadata(text_chunk: str) -> Tuple[Optional[str], Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Extrae títulos, subtítulos y sub-subtítulos de un fragmento de texto.\n",
    "\n",
    "    Args:\n",
    "        text_chunk (str): Fragmento de texto del cual extraer la metadata.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Optional[str], Optional[str], Optional[str]]: Título, subtítulo y sub-subtítulo encontrados (o None si no se encuentran).\n",
    "    \"\"\"\n",
    "    title_pattern = re.compile(r\"PART \\d+[-—]\\s*[A-Za-z0-9 ,.\\-]+\")\n",
    "    subtitle_pattern = re.compile(r\"Subpart [A-Z]—[A-Za-z0-9 ,\\\\-]+\")\n",
    "    sub_subtitle_pattern = re.compile(r\"§\\s*\\d+\\.\\d+\\s+[A-Za-z0-9 ,.\\-]+\")\n",
    "\n",
    "    title = title_pattern.search(text_chunk)\n",
    "    subtitle = subtitle_pattern.search(text_chunk)\n",
    "    sub_subtitle = sub_subtitle_pattern.search(text_chunk)\n",
    "\n",
    "    return (\n",
    "        title.group(0).strip() if title else None,\n",
    "        subtitle.group(0).strip() if subtitle else None,\n",
    "        sub_subtitle.group(0).strip() if sub_subtitle else None,\n",
    "    )\n",
    "\n",
    "def assign_metadata_to_chunks_with_context(chunks: List[str], max_previous_chunks: int = 100) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Asigna títulos, subtítulos y sub-subtítulos como metadata a cada chunk.\n",
    "\n",
    "    Args:\n",
    "        chunks (List[str]): Lista de fragmentos de texto.\n",
    "        max_previous_chunks (int): Número máximo de fragmentos previos a considerar para acumular metadata.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: Lista de fragmentos con metadata asignada.\n",
    "    \"\"\"\n",
    "    annotated_chunks = list()\n",
    "    for i in range(len(chunks)):\n",
    "        metadata_accumulated = {\"title\": None, \"subtitle\": None, \"sub_subtitle\": None}\n",
    "        for j in range(max(0, i - max_previous_chunks), i):\n",
    "            metadata_title, metadata_subtitle, metadata_subsubtitle = extract_metadata(chunks[j])\n",
    "            if metadata_title:\n",
    "                metadata_accumulated[\"title\"] = metadata_title\n",
    "            if metadata_subtitle:\n",
    "                metadata_accumulated[\"subtitle\"] = metadata_subtitle\n",
    "            if metadata_subsubtitle:\n",
    "                metadata_accumulated[\"sub_subtitle\"] = metadata_subsubtitle\n",
    "        annotated_chunks.append({\"chunk_text\": chunks[i], \"metadata\": metadata_accumulated.copy()})\n",
    "    return annotated_chunks\n",
    "\n",
    "# --- Qdrant Vector Store ---\n",
    "def create_qdrant_store(model_name: str, chunks: List[str]) -> QdrantVectorStore:\n",
    "    \"\"\"\n",
    "    Crea y devuelve un QdrantVectorStore a partir de un modelo de embeddings y una lista de chunks de texto.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Nombre del modelo de embeddings.\n",
    "        chunks (List[str]): Lista de fragmentos de texto.\n",
    "\n",
    "    Returns:\n",
    "        QdrantVectorStore: Objeto de almacenamiento Qdrant.\n",
    "    \"\"\"\n",
    "    # Crear embeddings con el modelo especificado\n",
    "    open_source_embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "    # Preparar documentos para Qdrant\n",
    "    documents_for_qdrant = [\n",
    "        Document(\n",
    "            page_content=item['chunk_text'], \n",
    "            metadata={\n",
    "                \"title\": item['metadata'].get('title', ''),\n",
    "                \"subtitle\": item['metadata'].get('subtitle', ''),\n",
    "                \"sub_subtitle\": item['metadata'].get('sub_subtitle', '')\n",
    "            }\n",
    "        )\n",
    "        for i, item in enumerate(chunks)  # Aquí se itera sobre los datos originales con metadatos\n",
    "    ]\n",
    "\n",
    "    # Crear la tienda de vectores en memoria\n",
    "    qdrant = QdrantVectorStore.from_documents(\n",
    "        documents_for_qdrant,\n",
    "        embedding=open_source_embeddings,\n",
    "        sparse_embedding=sparse_embeddings,\n",
    "        location=\":memory:\",  # Puedes cambiar la ubicación para persistencia\n",
    "        collection_name=\"my_documents\",\n",
    "        retrieval_mode=RetrievalMode.HYBRID,\n",
    "    )\n",
    "    \n",
    "    return qdrant \n",
    "\n",
    "def create_llm(model_name: str, temperature: float, openai_api_key: str) -> ChatOpenAI:\n",
    "    \"\"\"\n",
    "    Crea un modelo LLM utilizando los parámetros proporcionados.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Nombre del modelo a utilizar.\n",
    "        temperature (float): Grado de creatividad en las respuestas.\n",
    "        openai_api_key (str): Clave de API de OpenAI para la autenticación.\n",
    "\n",
    "    Returns:\n",
    "        ChatOpenAI: Una instancia del modelo configurado.\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(\n",
    "        model=model_name,\n",
    "        temperature=temperature,  # Ajusta la creatividad según sea necesario\n",
    "        openai_api_key=openai_api_key\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "def create_rag_chain(qdrant: QdrantVectorStore, llm: ChatOpenAI) -> QdrantVectorStore:\n",
    "    \"\"\"\n",
    "    Crea y devuelve una cadena RAG (Retrieval-Augmented Generation) utilizando LangChain.\n",
    "\n",
    "    Args:\n",
    "        model (str): Nombre del modelo OpenAI para la generación de texto.\n",
    "        openai_api_key (str): Clave de acceso a la API de OpenAI.\n",
    "        qdrant (QdrantVectorStore): Almacén de vectores configurado para recuperar documentos relevantes.\n",
    "        temperature (float): Nivel de creatividad del modelo en la generación de texto. \n",
    "\n",
    "    Returns:\n",
    "        rag_chain: La cadena RAG configurada para generación y recuperación.\n",
    "        retriever: El objeto retriever configurado para recuperar documentos relevantes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Descargar y configurar el prompt desde LangChain Hub\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # Función para formatear los documentos\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Configurar el retriever desde Qdrant\n",
    "    retriever = qdrant.as_retriever()\n",
    "\n",
    "    # Crear la cadena RAG\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return rag_chain,retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_factoid_qa_prompt():\n",
    "    \"\"\"\n",
    "    Genera un prompt mejorado para la creación de preguntas y respuestas factuales basado en un contexto.\n",
    "\n",
    "    Returns:\n",
    "        ChatPromptTemplate: Un objeto de plantilla de prompt para generación de QA.\n",
    "    \"\"\"\n",
    "    QA_generation_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Your task is to generate a *factoid question* and its corresponding *answer* based on the given context.\n",
    "\n",
    "    Here are the rules:\n",
    "    1. The *factoid question* must be directly answerable with a specific and concise piece of factual information from the context.\n",
    "    2. Avoid using phrases like \"according to the passage\" or \"based on the context\" in your question.\n",
    "    3. The question should resemble the style of queries typically entered in a search engine, focusing on clarity and relevance.\n",
    "    4. The context provided will have a maximum token limit of 200 to 300 tokens.                                                        \n",
    "\n",
    "    Please provide your response in the following format:\n",
    "\n",
    "    Output:::\n",
    "    Factoid question: (Your factoid question here)\n",
    "    Answer: (The answer to the factoid question here)\n",
    "\n",
    "    Here is the context:\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Output:::\n",
    "    \"\"\")\n",
    "    return QA_generation_prompt\n",
    "\n",
    "\n",
    "# Corregir la implementación del Passthrough\n",
    "class SimplePassthrough:\n",
    "    def __call__(self, inputs):\n",
    "        return inputs  # Devuelve los inputs directamente\n",
    "\n",
    "\n",
    "# Definimos una función para manejar un solo contexto\n",
    "def question_chain(context, prompt, config):\n",
    "    passthrough = SimplePassthrough()  # Instanciar el passthrough\n",
    "    passthrough_output = passthrough({\"context\": context})  # Paso directo\n",
    "\n",
    "    # Formatear el prompt\n",
    "    prompt_output = prompt.format(**passthrough_output)\n",
    "    \n",
    "    llm = create_llm(config[\"model\"], config[\"temperature\"], config[\"openai_api_key\"])\n",
    "\n",
    "    # Generar salida usando el modelo LLM\n",
    "    llm_output = llm.invoke(prompt_output)\n",
    "    \n",
    "    # Parsear la salida final\n",
    "    parsed_output = StrOutputParser().parse(llm_output)\n",
    "    \n",
    "    return parsed_output\n",
    "\n",
    "# Proceso para manejar varios documentos\n",
    "def process_multiple_docs(docs, prompt, config, num_samples=15):\n",
    "    # Seleccionar una muestra aleatoria de documentos\n",
    "    sampled_docs = random.sample(docs, num_samples)\n",
    "    sampled_docs_processed = [doc.page_content for doc in sampled_docs]\n",
    "    \n",
    "    # Procesar preguntas en batch\n",
    "    questions = [\n",
    "        question_chain(sampled_context, prompt, config)\n",
    "        for sampled_context in tqdm(sampled_docs_processed, desc=\"Processing questions\")\n",
    "    ]\n",
    "    \n",
    "    return questions\n",
    "\n",
    "def extract_questions_and_answers(data):\n",
    "    questions = list()\n",
    "    answers = list()\n",
    "    for message in data:\n",
    "        if hasattr(message, \"content\"):\n",
    "            content = message.content\n",
    "            if \"Factoid question:\" in content and \"Answer:\" in content:\n",
    "                # Extraer pregunta y respuesta\n",
    "                factoid_question = content.split(\"Factoid question:\")[1].split(\"\\nAnswer:\")[0].strip()\n",
    "                answer = content.split(\"Answer:\")[1].strip()\n",
    "                # Agregar a las listas correspondientes\n",
    "                questions.append(factoid_question)\n",
    "                answers.append(answer)\n",
    "    return questions, answers\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_rag_pipeline(rag_chain, retriever, questions, ground_truths):\n",
    "    \"\"\"\n",
    "    Realiza la inferencia con un pipeline RAG, evalúa los resultados y devuelve un DataFrame con las métricas.\n",
    "\n",
    "    Args:\n",
    "        rag_chain: El modelo RAG para generar respuestas.\n",
    "        retriever: El componente de recuperación para obtener contextos relevantes.\n",
    "        questions (list): Lista de preguntas para realizar la inferencia.\n",
    "        ground_truths (list): Lista de respuestas esperadas (ground truths) para evaluación.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame con los resultados de la evaluación.\n",
    "    \"\"\"\n",
    "    # Inicializar listas para almacenar respuestas y contextos\n",
    "    answers = list()\n",
    "    contexts = list()\n",
    "\n",
    "    # Inferencia para cada pregunta\n",
    "    for query in questions:\n",
    "        # Obtener respuesta del modelo\n",
    "        answers.append(rag_chain.invoke(query))\n",
    "        # Obtener contextos relevantes del retriever\n",
    "        relevant_docs = retriever.invoke(query)\n",
    "        contexts.append([doc.page_content for doc in relevant_docs])\n",
    "\n",
    "    # Crear conjunto de datos para evaluación\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truths\": ground_truths,\n",
    "        \"reference\": [gt[0] for gt in ground_truths]  # Agregar columna de referencia\n",
    "    })\n",
    "\n",
    "    # Evaluar el pipeline utilizando métricas de RAGAs\n",
    "    result = evaluate(\n",
    "        dataset=dataset, \n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Convertir resultados a DataFrame y devolver\n",
    "    df = result.to_pandas()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Cargando archivo: c:\\Users\\jomunozf\\Documents\\GitHub\\practicos-rag\\data\\CFR-2024-vol8.pdf\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/paraphrase-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/paraphrase-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a28c4b1d5b8415fb85655e08e7b9c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 29 files:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:   0%|          | 0/15 [00:00<?, ?it/s]C:\\Users\\jomunozf\\AppData\\Local\\Temp\\ipykernel_28752\\2023844394.py:49: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  llm_output = llm(prompt_output)\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:   7%|▋         | 1/15 [00:01<00:20,  1.47s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  13%|█▎        | 2/15 [00:02<00:16,  1.24s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  20%|██        | 3/15 [00:03<00:13,  1.09s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  27%|██▋       | 4/15 [00:04<00:13,  1.23s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  33%|███▎      | 5/15 [00:06<00:13,  1.30s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  40%|████      | 6/15 [00:07<00:10,  1.14s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  47%|████▋     | 7/15 [00:07<00:08,  1.04s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  53%|█████▎    | 8/15 [00:09<00:07,  1.07s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  60%|██████    | 9/15 [00:10<00:06,  1.01s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  67%|██████▋   | 10/15 [00:10<00:04,  1.04it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  73%|███████▎  | 11/15 [00:11<00:03,  1.02it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  80%|████████  | 12/15 [00:12<00:02,  1.02it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  87%|████████▋ | 13/15 [00:13<00:01,  1.02it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions:  93%|█████████▎| 14/15 [00:14<00:00,  1.04it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions: 100%|██████████| 15/15 [00:15<00:00,  1.06s/it]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a750f7a937a54561b61752a0eb47cd99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Respuesta: The procedures for milk pasteurization according to the regulations mentioned in the document involve heating every particle of milk or cream to a specific temperature and holding it there continuously for a specified time. The temperatures given in the document are 145 °F (63 °C) for 30 minutes or at least 161 °F for at least 15 seconds. These procedures are mandatory for all milk and milk products intended for direct human consumption.\n"
     ]
    }
   ],
   "source": [
    "# --- Flujo Principal ---\n",
    "def main():\n",
    "    os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "    load_dotenv()\n",
    "    config = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"openai_api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "        \"model_name\": \"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n",
    "        \"directory_path\": os.getcwd()+ \"\\\\data\\\\\",\n",
    "        \"buffer_size\": 2,\n",
    "        \"threshold\": 0.3,\n",
    "        \"max_previous_chunks\": 400,\n",
    "        \"temperature\": 0.7,\n",
    "        \"file_path\":  os.getcwd()+ \"\\\\data\\\\\" + \"CFR-2024-vol8.pdf\",\n",
    "        \"num_samples\":15,\n",
    "    }\n",
    "    #SuperRAG\n",
    "    pdf_texts = load_pdf_all_documents(config[\"directory_path\"])\n",
    "    cleaned_text = clean_text_and_exclude_sections(\" \".join(pdf_texts))\n",
    "    sentences = split_text_into_sentences(cleaned_text)\n",
    "    combined_sentences = combine_sentences(sentences, config[\"buffer_size\"])\n",
    "    distances = calculate_cosine_distances(combined_sentences, config[\"model_name\"])\n",
    "    chunks = split_into_chunks(combined_sentences, distances, config[\"threshold\"])\n",
    "    annotated_chunks = assign_metadata_to_chunks_with_context(chunks, config[\"max_previous_chunks\"])\n",
    "    qdrant_store = create_qdrant_store(config[\"model_name\"], annotated_chunks)\n",
    "    llm = create_llm(config[\"model\"], config[\"temperature\"], config[\"openai_api_key\"])\n",
    "    rag_chain, retriever = create_rag_chain(qdrant_store,llm)\n",
    "\n",
    "    #Raga\n",
    "    loader = PyPDFLoader(config[\"file_path\"])\n",
    "    docs = loader.load()\n",
    "    prompt = generate_factoid_qa_prompt()\n",
    "    new_questions = process_multiple_docs(docs, prompt, config, config[\"num_samples\"])\n",
    "    questions, ground_truths = extract_questions_and_answers(new_questions)\n",
    "    df_raga = evaluate_rag_pipeline(rag_chain, retriever, questions, ground_truths)\n",
    "    df_raga.to_csv(\"results.csv\", encoding = \"utf-8\", sep = \"|\")\n",
    "\n",
    "\n",
    "    question = \"What are the procedures for milk pasteurization according to the regulations mentioned in the document?\"\n",
    "    response = rag_chain.invoke(question)\n",
    "    logging.info(f\"Respuesta: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Carga un archivo PDF y devuelve las primeras páginas como documentos.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Ruta al archivo PDF.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de documentos extraídos del PDF.\n",
    "    \"\"\"\n",
    "    # Cargar variables de entorno si son necesarias\n",
    "    load_dotenv()\n",
    "\n",
    "    # Cargar el archivo PDF\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    return docs  # Devuelve todas las páginas como lista\n",
    "\n",
    "def split_pdf_documents(docs, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Divide un documento PDF en fragmentos de texto.\n",
    "\n",
    "    Args:\n",
    "        docs (list): Lista de documentos cargados desde un PDF.\n",
    "        chunk_size (int): Tamaño de cada fragmento de texto en caracteres. Default es 1000.\n",
    "        chunk_overlap (int): Cantidad de solapamiento entre fragmentos. Default es 200.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de fragmentos de texto extraídos del PDF.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    return splits  # Devuelve los fragmentos divididos\n",
    "\n",
    "\n",
    "# main\n",
    "docs = load_pdf(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entorno_prueba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
